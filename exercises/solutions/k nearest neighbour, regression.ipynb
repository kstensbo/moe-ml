{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$ nearest neighbours for regression\n",
    "\n",
    "The [$k$ nearest neighbours](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) method is one of the simplest machine learning methods, yet it often performs incredibly well.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Given a new datapoint $x_i$, find the $k$ closest datapoints in the dataset $X$.\n",
    "2. Estimate an output value as the mean of the outputs of the $k$ closest datapoints.\n",
    "\n",
    "The number of neighbours, $k$, to look at typically lies in the range of $[1, \\dots, 30]$. The question is how to select the right $k$ for a given task. One way to do that is by using *cross validation*.\n",
    "\n",
    "## Cross validation\n",
    "[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) (CV) is used both for testing the performance of a method and for selecting hyperparameters, such as the number of nearest neighbours, $k$. The most common version, $K$-fold CV, divides the dataset into $K$ partitions and uses $K-1$ as the training set and the remaining partition as test set. This is repeated $K$ times, each time using a different partition as test set. It is *very* important to randomise the dataset before partitioning it, in case the data points have been sorted.\n",
    "\n",
    "Typical values of $K$ are 3, 5, and 10. The higher the better, but also more time is required to run the CV, so in practice the value is chosen as a trade-off between statistical significance and time constraints.\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "1. Implement $k$ nearest neighbours, but for now just fix the value of $k$ manually.\n",
    "2. Implement cross validation.\n",
    "3. Use cross validation to find the best $k$ for the dataset.\n",
    "\n",
    "Test the implemented methods on the data in `ex1.dat`, `ex2.dat`, and `ex3.dat`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load data from exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"../data/ex1.dat\")\n",
    "X = data[:, 0]\n",
    "y = data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$ nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to predict the value at $x_0 = 2$. This means we need to look for nearest neighbours around this position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position to look for nearest neighbours at:\n",
    "x0 = 2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "ax.axvline(x0, color=\"C1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will predict the value at $x_0=2$ based on the $k=3$ nearest neighbours. First, we need to locate these, then compute the mean of their output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3    # number of neighbours to look for\n",
    "\n",
    "# Compute the distance from all points to x0\n",
    "dists = np.abs(X - x0)\n",
    "\n",
    "# Find the indices of the k closest:\n",
    "idx = np.argsort(dists)[:k]\n",
    "print(\"k = {} closest data points: x = {}\".format(k, X[idx]))\n",
    "\n",
    "# Find mean of the outputs:\n",
    "prediction = np.mean(y[idx])\n",
    "print(\"Prediction for x0 = {}: y0 = {}\".format(x0, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's highlight the found points and our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data, but change the colour if the point is one of the nearest neighbours:\n",
    "ax.scatter(X, y, color=[\"C1\" if i in idx else \"C0\" for i in range(len(X))])\n",
    "\n",
    "# Plot our prediction:\n",
    "ax.scatter(x0, prediction, s=60, marker=\"x\", color=\"C1\")\n",
    "\n",
    "ax.axvline(x0, color=\"C1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$ nearest neighbours as a function\n",
    "\n",
    "Let's wrap the above in an easy-to-use function that we can use on an entire list of points to make predictions at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(k, xf, X, y):\n",
    "    \"\"\"\n",
    "    Compute the k nearest neighbours' estimate for all x's in `xf`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Allocate an array for predictions:\n",
    "    yf = np.zeros(len(xf))\n",
    "    \n",
    "    for i, x in enumerate(xf):\n",
    "        # For each point we want a prediction for, do a\n",
    "        # nearest neighbours estimate as above:\n",
    "        dists = np.abs(X - x)\n",
    "        idx = np.argsort(dists)[:k]\n",
    "        yf[i] = np.mean(y[idx])\n",
    "        \n",
    "    return yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, it is easy to compute the $k$-NN estimate for a long range of points that we can then draw a line through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input points and compute outputs:\n",
    "xf = np.linspace(0, 10, 200)\n",
    "yf = knn(3, xf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "ax.plot(xf, yf, ls=\"-\", color=\"C1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "To implement $K$-fold cross validation, we need to identify $K$ random splits of (roughly) equal size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds to use\n",
    "K = 3\n",
    "\n",
    "fold_size = len(X)//K\n",
    "\n",
    "# Create indices and shuffle them\n",
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Divide list of indices into chunks of size fold_size\n",
    "folds = []\n",
    "for i in range(K):\n",
    "    folds.append(idx[i * fold_size:(i + 1) * fold_size])\n",
    "\n",
    "folds = np.array(folds)\n",
    "print(\"Indices for each fold:\\n\", folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these indices to construct a training set and a test set and test our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, fold in enumerate(folds):\n",
    "    # Get indices for which folds to use for training:\n",
    "    train_idx = np.delete(np.arange(K, dtype=int), f)\n",
    "    # Construct training set by concatenating the indices:\n",
    "    train_folds = np.concatenate(folds[train_idx])\n",
    "    \n",
    "    # Select data for training and tes\n",
    "    X_train = X[train_folds]\n",
    "    y_train = y[train_folds]\n",
    "    X_test = X[fold]\n",
    "    y_test = y[fold]\n",
    "\n",
    "    # Get predictions for each test point:\n",
    "    yp = knn(k, X_test, X_train, y_train)\n",
    "\n",
    "    # Compute the RMSE\n",
    "    RMSE = np.sqrt(np.mean(np.array(yp - y_test)**2))\n",
    "    \n",
    "    print(\"Fold {}: RMSE = {:g}\".format(f, RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CV to determine the optimal $k$\n",
    "\n",
    "We can now use such a CV to test how many neighbours would be the optimal choice for our $k$-NN and the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_err = []    # list to store the mean RMSE for each k\n",
    "k_range = np.arange(1,20, dtype=int)\n",
    "\n",
    "for k in k_range:\n",
    "    rmses = []\n",
    "    \n",
    "    for f, fold in enumerate(folds):\n",
    "        # Get indices for which folds to use for training:\n",
    "        train_idx = np.delete(np.arange(K, dtype=int), f)\n",
    "        # Construct training set by concatenating the indices:\n",
    "        train_folds = np.concatenate(folds[train_idx])\n",
    "\n",
    "        # Select data for training and tes\n",
    "        X_train = X[train_folds]\n",
    "        y_train = y[train_folds]\n",
    "        X_test = X[fold]\n",
    "        y_test = y[fold]\n",
    "\n",
    "        # Get predictions for each test point:\n",
    "        yp = knn(k, X_test, X_train, y_train)\n",
    "\n",
    "        # Compute the RMSE and store it\n",
    "        RMSE = np.sqrt(np.mean(np.array(yp - y_test)**2))\n",
    "        rmses.append(RMSE)\n",
    "\n",
    "    print(\"k = {}: mean RMSE = {:g}\".format(k, np.mean(rmses)))\n",
    "\n",
    "    # Save mean and std of the RMSE for each k\n",
    "    CV_err.append([np.mean(rmses), np.std(rmses)])\n",
    "CV_err = np.array(CV_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can now locate the $k$ with the smalles RMSE and use that for further predictions on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_lowest_RMSE = np.argmin(CV_err[:,0])\n",
    "\n",
    "print(\"Smallest RMSE of {:g} is achieved for k = {}.\".format(CV_err[idx_of_lowest_RMSE,0], \n",
    "                                                             k_range[idx_of_lowest_RMSE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualise the mean and std of the RMSE for each $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(k_range, CV_err[:,0], yerr=CV_err[:,1])\n",
    "\n",
    "ax.set_xticks(k_range)   # Make tick marks for all k\n",
    "ax.set_xlabel(\"$k$\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As seen in the plot, the best $k$ lies at the very low end of the range, but the uncertainties are also too large to make a conclusive decision on what $k$ to use. In such a situation, one might try to reduce the uncertainties by acquiring more data or increasing the number of folds in the CV. In the end, one always picks the $k$ with the smallest mean RMSE, regardless of large the uncertainties are."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
