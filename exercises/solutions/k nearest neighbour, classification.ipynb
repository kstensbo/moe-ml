{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$ nearest neighbours for classification\n",
    "\n",
    "The [$k$ nearest neighbours](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) method is also very useful for classification.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Given a new datapoint $x_i$, find the $k$ closest datapoints in the dataset $X$.\n",
    "2. Estimate the class of the point as the most common class among the $k$ closest datapoints.\n",
    "\n",
    "As for the regression version, the chosen value of $k$ is typically rather small and chosen using cross validation.\n",
    "\n",
    "\n",
    "## Measuring classification error\n",
    "\n",
    "When doing regression, one often uses the RMSE to measure how accurate a model is. The RMSE uses the difference between predictions, but for (multi-class) classification, the difference is meaningless - either you make a correct prediction or you do not.\n",
    "\n",
    "For this reason, the *0-1 loss* is often used:\n",
    "$$\n",
    "L(\\hat{y}, y) = \n",
    "\\begin{cases}\n",
    "    1 & \\text{if } \\hat{y} \\neq y\\\\\n",
    "    0 & \\text{if } \\hat{y} = y\n",
    "\\end{cases}\n",
    "$$\n",
    "where $y$ is the true label and $\\hat{y}$ is the predicted label.\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Implement $k$ nearest neighbours classification and test it by fixing the value of $k$ manually.\n",
    "2. Use cross validation to find the best $k$ for the dataset.\n",
    "\n",
    "Test the implemented methods on the data in `ex4.dat` and `ex5.dat`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Load exercise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"../data/ex4.dat\")\n",
    "X = data[:, :2]               # coordinates\n",
    "y = data[:, 2].astype(int)    # classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[y == 0, 0], X[y == 0, 1], color=\"C0\", label=\"Class 1\")\n",
    "ax.scatter(X[y == 1, 0], X[y == 1, 1], color=\"C1\", label=\"Class 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$ nearest neighbours\n",
    "\n",
    "Let's modify the function from before to find the most common class among the neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(k, xf, X, y):\n",
    "    \"\"\"\n",
    "    Compute the k nearest neighbours' estimate for all x's in `xf`.\n",
    "    X: training set inputs.\n",
    "    y: training set labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Allocate an array for predictions:\n",
    "    yf = np.zeros(len(xf), dtype=int)\n",
    "    \n",
    "    for i, x in enumerate(xf):\n",
    "        # For each point we want a prediction for, calculate the distance\n",
    "        # to all training points...\n",
    "        dists = np.linalg.norm(np.subtract(X, x), axis=1)\n",
    "        \n",
    "        # ... locate the k nearest...\n",
    "        idx = np.argsort(dists)[:k]\n",
    "        \n",
    "        # ... and find the most common class.\n",
    "        unique, count = np.unique(y[idx], return_counts=True)\n",
    "        yf[i] = unique[np.argmax(count)]\n",
    "        \n",
    "    return yf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function, it is easy to compute the $k$-NN estimate for a range of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "# Create input points and compute outputs:\n",
    "xf = np.random.rand(10, 2)*2 - 1\n",
    "yf = knn(k, xf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Plot the training data:\n",
    "ax.scatter(X[y == 0, 0], X[y == 0, 1], color=\"C0\", label=\"Class 1\")\n",
    "ax.scatter(X[y == 1, 0], X[y == 1, 1], color=\"C1\", label=\"Class 2\")\n",
    "\n",
    "# Plot the predictions:\n",
    "ax.scatter(xf[:,0], xf[:,1], marker=\"x\", s=60, color=[\"C{}\".format(c) for c in yf])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the plot even fancier by plotting the decision regions as a background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "\n",
    "# Define resolution for the background:\n",
    "nx, ny = (100, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# Plot the training data:\n",
    "ax.scatter(X[y == 0, 0], X[y == 0, 1], color=\"C0\", label=\"Class 1\")\n",
    "ax.scatter(X[y == 1, 0], X[y == 1, 1], color=\"C1\", label=\"Class 2\")\n",
    "\n",
    "# Get plot bounding box:\n",
    "l = ax.axis()\n",
    "\n",
    "# Create grid inside the plotting area:\n",
    "X1, X2 = np.meshgrid(np.linspace(l[0], l[1], nx),\n",
    "                     np.linspace(l[2], l[3], ny),\n",
    "                    )\n",
    "\n",
    "# Change shape to match what our knn function expects:\n",
    "xlist = np.dstack([X1, X2]).reshape(nx*ny, 2)\n",
    "\n",
    "# Make predictions and change shape back to a grid:\n",
    "C = knn(k, xlist, X, y).reshape(nx, ny)\n",
    "\n",
    "# Create a background image using a colour scheme with lighter colours:\n",
    "background = plt.cm.tab20c(C*4 + 3)\n",
    "\n",
    "\n",
    "# Plot the predictions:\n",
    "ax.imshow(background, extent=l, origin=\"lower\", interpolation='none', zorder=-100)\n",
    "\n",
    "# Adjust plot area:\n",
    "ax.axis(l)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Again, we use $K$-fold CV to find the best value for $k$. First, we create random splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds to use\n",
    "K = 10\n",
    "\n",
    "fold_size = len(X)//K\n",
    "\n",
    "# Create indices and shuffle them\n",
    "idx = np.arange(len(X))\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Divide list of indices into chunks of size fold_size\n",
    "folds = []\n",
    "for i in range(K):\n",
    "    folds.append(idx[i * fold_size:(i + 1) * fold_size])\n",
    "\n",
    "folds = np.array(folds)\n",
    "print(\"Indices for each fold:\\n\", folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these splits, we can test how many neighbours would be the optimal choice for our $k$-NN and the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_err = []    # list to store the mean 0-1 loss for each k\n",
    "k_range = np.arange(1,20, dtype=int)\n",
    "\n",
    "for k in k_range:\n",
    "    losses = []\n",
    "    \n",
    "    for f, fold in enumerate(folds):\n",
    "        # Get indices for which folds to use for training:\n",
    "        train_folds = np.delete(np.arange(K, dtype=int), f)\n",
    "        # Construct training set by concatenating the indices:\n",
    "        train_idx = np.concatenate(folds[train_folds])\n",
    "        \n",
    "        # Select data for training and tes\n",
    "        X_train = X[train_idx]\n",
    "        y_train = y[train_idx]\n",
    "        X_test = X[fold]\n",
    "        y_test = y[fold]\n",
    "\n",
    "        # Get predictions for each test point:\n",
    "        yp = knn(k, X_test, X_train, y_train)\n",
    "\n",
    "        # Compute the 0-1 loss and store it\n",
    "        losses.append(np.sum(yp != y_test))\n",
    "\n",
    "    print(\"k = {}: mean 0-1 loss = {:g}\".format(k, np.mean(losses)))\n",
    "\n",
    "    # Save mean and std of the 0-1 loss for each k\n",
    "    CV_err.append([np.mean(losses), np.std(losses)])\n",
    "CV_err = np.array(CV_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can now locate the $k$ with the smalles 0-1 loss and use that for further predictions on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_of_lowest_loss = np.argmin(CV_err[:,0])\n",
    "\n",
    "print(\"Smallest loss of {:g} is achieved for k = {}.\".format(CV_err[idx_of_lowest_loss, 0], \n",
    "                                                             k_range[idx_of_lowest_loss]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualise the mean and std of the loss for each $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(k_range, CV_err[:,0], yerr=CV_err[:,1])\n",
    "\n",
    "ax.set_xticks(k_range)   # Make tick marks for all k\n",
    "ax.set_xlabel(\"$k$\")\n",
    "ax.set_ylabel(\"0-1 loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Like for $k$-NN regression, lower values of $k$ are preferred. For the data in `ex4.dat`, many values of $k$ will give no error, i.e. a perfect fit. That is because the two classes in the dataset are well separated. For datasets where the classes overlap, or nearly overlap as in `ex5.dat`, the classification error will never be zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
